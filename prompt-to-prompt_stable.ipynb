{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "0zdT1vziHQNW"
      },
      "source": [
        "## Copyright 2022 Google LLC. Double-click for license information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TBhwVd11HQNX"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y diffusers\n",
        "!pip install diffusers==0.11.1\n",
        "print(\"✅ Installed compatible diffusers version\")\n",
        "print(\"⚠️ IMPORTANT: Click 'Runtime' → 'Restart runtime' now!\")"
      ],
      "metadata": {
        "id": "X0ucC6KwL8A0",
        "outputId": "562eefa2-3093-4158-8f9e-17a61a30cd81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: diffusers 0.11.1\n",
            "Uninstalling diffusers-0.11.1:\n",
            "  Successfully uninstalled diffusers-0.11.1\n",
            "Collecting diffusers==0.11.1\n",
            "  Using cached diffusers-0.11.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (2.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.11.1) (11.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->diffusers==0.11.1) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.11.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.11.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.11.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.11.1) (2025.10.5)\n",
            "Using cached diffusers-0.11.1-py3-none-any.whl (524 kB)\n",
            "Installing collected packages: diffusers\n",
            "Successfully installed diffusers-0.11.1\n",
            "✅ Installed compatible diffusers version\n",
            "⚠️ IMPORTANT: Click 'Runtime' → 'Restart runtime' now!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all compatible versions together\n",
        "!pip uninstall -y diffusers transformers huggingface-hub\n",
        "!pip install diffusers==0.11.1 transformers==4.25.1 huggingface-hub==0.10.1 accelerate\n",
        "\n",
        "print(\"✅ Installed compatible versions!\")\n",
        "print(\"⚠️ CRITICAL: Go to Runtime → Restart runtime NOW\")\n",
        "print(\"Then run all cells from the beginning\")"
      ],
      "metadata": {
        "id": "WRegi7AmMRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate"
      ],
      "metadata": {
        "id": "kkGn7iO0LB0X",
        "outputId": "a2b8bbef-0abf-4251-9673-df9580c50846",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.11.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.10.0->diffusers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository to get the helper files\n",
        "!git clone https://github.com/google/prompt-to-prompt.git\n",
        "\n",
        "# Add the repository to Python path so we can import from it\n",
        "import sys\n",
        "sys.path.append('/content/prompt-to-prompt')"
      ],
      "metadata": {
        "id": "5fnx_o3OLXAX",
        "outputId": "e8ae98cf-b6a7-4e0c-91a9-1d354a1aa14d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'prompt-to-prompt' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "gHXqvZO7HQNY"
      },
      "source": [
        "## Prompt-to-Prompt with Stable Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "k6xLpxfZHQNY",
        "outputId": "6a21501d-1cdb-445f-c4e3-acfbc1b0dc12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3719771559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnnf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mget_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipeline_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffusionPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     from .pipelines import (\n\u001b[1;32m     48\u001b[0m         \u001b[0mDanceDiffusionPipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/pipeline_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_modules_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhub_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LOW_CPU_MEM_USAGE_DEFAULT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/diffusers/dynamic_modules_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.12/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch.nn.functional as nnf\n",
        "import numpy as np\n",
        "import abc\n",
        "import ptp_utils\n",
        "import seq_aligner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "n_dApsQCHQNY"
      },
      "source": [
        "For loading the Stable Diffusion using Diffusers, follow the instuctions https://huggingface.co/blog/stable_diffusion and update ```MY_TOKEN``` with your token.\n",
        "Set ```LOW_RESOURCE``` to ```True``` for running on 12GB GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [],
        "id": "9-Hc5OyLHQNY"
      },
      "outputs": [],
      "source": [
        "MY_TOKEN = '<replace with your token>'\n",
        "LOW_RESOURCE = False\n",
        "NUM_DIFFUSION_STEPS = 50\n",
        "GUIDANCE_SCALE = 7.5\n",
        "MAX_NUM_WORDS = 77\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_auth_token=MY_TOKEN).to(device)\n",
        "tokenizer = ldm_stable.tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "oQZJMyXZHQNY"
      },
      "source": [
        "## Prompt-to-Prompt Attnetion Controllers\n",
        "Our main logic is implemented in the `forward` call in an `AttentionControl` object.\n",
        "The forward is called in each attention layer of the diffusion model and it can modify the input attnetion weights `attn`.\n",
        "\n",
        "`is_cross`, `place_in_unet in (\"down\", \"mid\", \"up\")`, `AttentionControl.cur_step` help us track the exact attention layer and timestamp during the diffusion iference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgXU4KPHQNZ"
      },
      "outputs": [],
      "source": [
        "class LocalBlend:\n",
        "\n",
        "    def __call__(self, x_t, attention_store):\n",
        "        k = 1\n",
        "        maps = attention_store[\"down_cross\"][2:4] + attention_store[\"up_cross\"][:3]\n",
        "        maps = [item.reshape(self.alpha_layers.shape[0], -1, 1, 16, 16, MAX_NUM_WORDS) for item in maps]\n",
        "        maps = torch.cat(maps, dim=1)\n",
        "        maps = (maps * self.alpha_layers).sum(-1).mean(1)\n",
        "        mask = nnf.max_pool2d(maps, (k * 2 + 1, k * 2 +1), (1, 1), padding=(k, k))\n",
        "        mask = nnf.interpolate(mask, size=(x_t.shape[2:]))\n",
        "        mask = mask / mask.max(2, keepdims=True)[0].max(3, keepdims=True)[0]\n",
        "        mask = mask.gt(self.threshold)\n",
        "        mask = (mask[:1] + mask[1:]).float()\n",
        "        x_t = x_t[:1] + mask * (x_t - x_t[:1])\n",
        "        return x_t\n",
        "\n",
        "    def __init__(self, prompts: List[str], words: [List[List[str]]], threshold=.3):\n",
        "        alpha_layers = torch.zeros(len(prompts),  1, 1, 1, 1, MAX_NUM_WORDS)\n",
        "        for i, (prompt, words_) in enumerate(zip(prompts, words)):\n",
        "            if type(words_) is str:\n",
        "                words_ = [words_]\n",
        "            for word in words_:\n",
        "                ind = ptp_utils.get_word_inds(prompt, word, tokenizer)\n",
        "                alpha_layers[i, :, :, :, :, ind] = 1\n",
        "        self.alpha_layers = alpha_layers.to(device)\n",
        "        self.threshold = threshold\n",
        "\n",
        "\n",
        "class AttentionControl(abc.ABC):\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "        return x_t\n",
        "\n",
        "    def between_steps(self):\n",
        "        return\n",
        "\n",
        "    @property\n",
        "    def num_uncond_att_layers(self):\n",
        "        return self.num_att_layers if LOW_RESOURCE else 0\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __call__(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        if self.cur_att_layer >= self.num_uncond_att_layers:\n",
        "            if LOW_RESOURCE:\n",
        "                attn = self.forward(attn, is_cross, place_in_unet)\n",
        "            else:\n",
        "                h = attn.shape[0]\n",
        "                attn[h // 2:] = self.forward(attn[h // 2:], is_cross, place_in_unet)\n",
        "        self.cur_att_layer += 1\n",
        "        if self.cur_att_layer == self.num_att_layers + self.num_uncond_att_layers:\n",
        "            self.cur_att_layer = 0\n",
        "            self.cur_step += 1\n",
        "            self.between_steps()\n",
        "        return attn\n",
        "\n",
        "    def reset(self):\n",
        "        self.cur_step = 0\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cur_step = 0\n",
        "        self.num_att_layers = -1\n",
        "        self.cur_att_layer = 0\n",
        "\n",
        "class EmptyControl(AttentionControl):\n",
        "\n",
        "    def forward (self, attn, is_cross: bool, place_in_unet: str):\n",
        "        return attn\n",
        "\n",
        "\n",
        "class AttentionStore(AttentionControl):\n",
        "\n",
        "    @staticmethod\n",
        "    def get_empty_store():\n",
        "        return {\"down_cross\": [], \"mid_cross\": [], \"up_cross\": [],\n",
        "                \"down_self\": [],  \"mid_self\": [],  \"up_self\": []}\n",
        "\n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        key = f\"{place_in_unet}_{'cross' if is_cross else 'self'}\"\n",
        "        if attn.shape[1] <= 32 ** 2:  # avoid memory overhead\n",
        "            self.step_store[key].append(attn)\n",
        "        return attn\n",
        "\n",
        "    def between_steps(self):\n",
        "        if len(self.attention_store) == 0:\n",
        "            self.attention_store = self.step_store\n",
        "        else:\n",
        "            for key in self.attention_store:\n",
        "                for i in range(len(self.attention_store[key])):\n",
        "                    self.attention_store[key][i] += self.step_store[key][i]\n",
        "        self.step_store = self.get_empty_store()\n",
        "\n",
        "    def get_average_attention(self):\n",
        "        average_attention = {key: [item / self.cur_step for item in self.attention_store[key]] for key in self.attention_store}\n",
        "        return average_attention\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        super(AttentionStore, self).reset()\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AttentionStore, self).__init__()\n",
        "        self.step_store = self.get_empty_store()\n",
        "        self.attention_store = {}\n",
        "\n",
        "\n",
        "class AttentionControlEdit(AttentionStore, abc.ABC):\n",
        "\n",
        "    def step_callback(self, x_t):\n",
        "        if self.local_blend is not None:\n",
        "            x_t = self.local_blend(x_t, self.attention_store)\n",
        "        return x_t\n",
        "\n",
        "    def replace_self_attention(self, attn_base, att_replace):\n",
        "        if att_replace.shape[2] <= 16 ** 2:\n",
        "            return attn_base.unsqueeze(0).expand(att_replace.shape[0], *attn_base.shape)\n",
        "        else:\n",
        "            return att_replace\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
        "        super(AttentionControlEdit, self).forward(attn, is_cross, place_in_unet)\n",
        "        if is_cross or (self.num_self_replace[0] <= self.cur_step < self.num_self_replace[1]):\n",
        "            h = attn.shape[0] // (self.batch_size)\n",
        "            attn = attn.reshape(self.batch_size, h, *attn.shape[1:])\n",
        "            attn_base, attn_repalce = attn[0], attn[1:]\n",
        "            if is_cross:\n",
        "                alpha_words = self.cross_replace_alpha[self.cur_step]\n",
        "                attn_repalce_new = self.replace_cross_attention(attn_base, attn_repalce) * alpha_words + (1 - alpha_words) * attn_repalce\n",
        "                attn[1:] = attn_repalce_new\n",
        "            else:\n",
        "                attn[1:] = self.replace_self_attention(attn_base, attn_repalce)\n",
        "            attn = attn.reshape(self.batch_size * h, *attn.shape[2:])\n",
        "        return attn\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int,\n",
        "                 cross_replace_steps: Union[float, Tuple[float, float], Dict[str, Tuple[float, float]]],\n",
        "                 self_replace_steps: Union[float, Tuple[float, float]],\n",
        "                 local_blend: Optional[LocalBlend]):\n",
        "        super(AttentionControlEdit, self).__init__()\n",
        "        self.batch_size = len(prompts)\n",
        "        self.cross_replace_alpha = ptp_utils.get_time_words_attention_alpha(prompts, num_steps, cross_replace_steps, tokenizer).to(device)\n",
        "        if type(self_replace_steps) is float:\n",
        "            self_replace_steps = 0, self_replace_steps\n",
        "        self.num_self_replace = int(num_steps * self_replace_steps[0]), int(num_steps * self_replace_steps[1])\n",
        "        self.local_blend = local_blend\n",
        "\n",
        "class AttentionReplace(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        return torch.einsum('hpw,bwn->bhpn', attn_base, self.mapper)\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionReplace, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper = seq_aligner.get_replacement_mapper(prompts, tokenizer).to(device)\n",
        "\n",
        "\n",
        "class AttentionRefine(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        attn_base_replace = attn_base[:, :, self.mapper].permute(2, 0, 1, 3)\n",
        "        attn_replace = attn_base_replace * self.alphas + att_replace * (1 - self.alphas)\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float,\n",
        "                 local_blend: Optional[LocalBlend] = None):\n",
        "        super(AttentionRefine, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.mapper, alphas = seq_aligner.get_refinement_mapper(prompts, tokenizer)\n",
        "        self.mapper, alphas = self.mapper.to(device), alphas.to(device)\n",
        "        self.alphas = alphas.reshape(alphas.shape[0], 1, 1, alphas.shape[1])\n",
        "\n",
        "\n",
        "class AttentionReweight(AttentionControlEdit):\n",
        "\n",
        "    def replace_cross_attention(self, attn_base, att_replace):\n",
        "        if self.prev_controller is not None:\n",
        "            attn_base = self.prev_controller.replace_cross_attention(attn_base, att_replace)\n",
        "        attn_replace = attn_base[None, :, :, :] * self.equalizer[:, None, None, :]\n",
        "        return attn_replace\n",
        "\n",
        "    def __init__(self, prompts, num_steps: int, cross_replace_steps: float, self_replace_steps: float, equalizer,\n",
        "                local_blend: Optional[LocalBlend] = None, controller: Optional[AttentionControlEdit] = None):\n",
        "        super(AttentionReweight, self).__init__(prompts, num_steps, cross_replace_steps, self_replace_steps, local_blend)\n",
        "        self.equalizer = equalizer.to(device)\n",
        "        self.prev_controller = controller\n",
        "\n",
        "\n",
        "def get_equalizer(text: str, word_select: Union[int, Tuple[int, ...]], values: Union[List[float],\n",
        "                  Tuple[float, ...]]):\n",
        "    if type(word_select) is int or type(word_select) is str:\n",
        "        word_select = (word_select,)\n",
        "    equalizer = torch.ones(len(values), 77)\n",
        "    values = torch.tensor(values, dtype=torch.float32)\n",
        "    for word in word_select:\n",
        "        inds = ptp_utils.get_word_inds(text, word, tokenizer)\n",
        "        equalizer[:, inds] = values\n",
        "    return equalizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PIF4MzTsHQNZ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def aggregate_attention(attention_store: AttentionStore, res: int, from_where: List[str], is_cross: bool, select: int):\n",
        "    out = []\n",
        "    attention_maps = attention_store.get_average_attention()\n",
        "    num_pixels = res ** 2\n",
        "    for location in from_where:\n",
        "        for item in attention_maps[f\"{location}_{'cross' if is_cross else 'self'}\"]:\n",
        "            if item.shape[1] == num_pixels:\n",
        "                cross_maps = item.reshape(len(prompts), -1, res, res, item.shape[-1])[select]\n",
        "                out.append(cross_maps)\n",
        "    out = torch.cat(out, dim=0)\n",
        "    out = out.sum(0) / out.shape[0]\n",
        "    return out.cpu()\n",
        "\n",
        "\n",
        "def show_cross_attention(attention_store: AttentionStore, res: int, from_where: List[str], select: int = 0):\n",
        "    tokens = tokenizer.encode(prompts[select])\n",
        "    decoder = tokenizer.decode\n",
        "    attention_maps = aggregate_attention(attention_store, res, from_where, True, select)\n",
        "    images = []\n",
        "    for i in range(len(tokens)):\n",
        "        image = attention_maps[:, :, i]\n",
        "        image = 255 * image / image.max()\n",
        "        image = image.unsqueeze(-1).expand(*image.shape, 3)\n",
        "        image = image.numpy().astype(np.uint8)\n",
        "        image = np.array(Image.fromarray(image).resize((256, 256)))\n",
        "        image = ptp_utils.text_under_image(image, decoder(int(tokens[i])))\n",
        "        images.append(image)\n",
        "    ptp_utils.view_images(np.stack(images, axis=0))\n",
        "\n",
        "\n",
        "def show_self_attention_comp(attention_store: AttentionStore, res: int, from_where: List[str],\n",
        "                        max_com=10, select: int = 0):\n",
        "    attention_maps = aggregate_attention(attention_store, res, from_where, False, select).numpy().reshape((res ** 2, res ** 2))\n",
        "    u, s, vh = np.linalg.svd(attention_maps - np.mean(attention_maps, axis=1, keepdims=True))\n",
        "    images = []\n",
        "    for i in range(max_com):\n",
        "        image = vh[i].reshape(res, res)\n",
        "        image = image - image.min()\n",
        "        image = 255 * image / image.max()\n",
        "        image = np.repeat(np.expand_dims(image, axis=2), 3, axis=2).astype(np.uint8)\n",
        "        image = Image.fromarray(image).resize((256, 256))\n",
        "        image = np.array(image)\n",
        "        images.append(image)\n",
        "    ptp_utils.view_images(np.concatenate(images, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6Ngz53hgHQNZ"
      },
      "outputs": [],
      "source": [
        "def run_and_display(prompts, controller, latent=None, run_baseline=False, generator=None):\n",
        "    if run_baseline:\n",
        "        print(\"w.o. prompt-to-prompt\")\n",
        "        images, latent = run_and_display(prompts, EmptyControl(), latent=latent, run_baseline=False, generator=generator)\n",
        "        print(\"with prompt-to-prompt\")\n",
        "    images, x_t = ptp_utils.text2image_ldm_stable(ldm_stable, prompts, controller, latent=latent, num_inference_steps=NUM_DIFFUSION_STEPS, guidance_scale=GUIDANCE_SCALE, generator=generator, low_resource=LOW_RESOURCE)\n",
        "    ptp_utils.view_images(images)\n",
        "    return images, x_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "6j78t-GzHQNZ"
      },
      "source": [
        "## Cross-Attention Visualization\n",
        "First let's generate an image and visualize the cross-attention maps for each word in the prompt.\n",
        "Notice, we normalize each map to 0-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lh4g9pBvHQNZ"
      },
      "outputs": [],
      "source": [
        "g_cpu = torch.Generator().manual_seed(8888)\n",
        "prompts = [\"A painting of a squirrel eating a burger\"]\n",
        "controller = AttentionStore()\n",
        "image, x_t = run_and_display(prompts, controller, latent=None, run_baseline=False, generator=g_cpu)\n",
        "show_cross_attention(controller, res=16, from_where=(\"up\", \"down\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "HwNUe2eCHQNZ"
      },
      "source": [
        "## Replacement edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nMw6MsO2HQNZ"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A painting of a lion eating a burger\"]\n",
        "\n",
        "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8, self_replace_steps=0.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OeGE-FlhHQNZ"
      },
      "source": [
        "### Modify Cross-Attention injection #steps for specific words\n",
        "Next, we can reduce the restriction on our lion by reducing the number of cross-attention injection with respect to the word \"lion\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FAv4whM6HQNZ"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A painting of a lion eating a burger\"]\n",
        "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
        "                              self_replace_steps=0.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jmJ1u_4uHQNZ"
      },
      "source": [
        "### Local Edit\n",
        "Lastly, if we want to preseve the original burger, we can apply a local edit with respect to the squirrel and the lion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tr56Uo-uHQNZ"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A painting of a lion eating a burger\"]\n",
        "lb = LocalBlend(prompts, (\"squirrel\", \"lion\"))\n",
        "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
        "                              cross_replace_steps={\"default_\": 1., \"lion\": .4},\n",
        "                              self_replace_steps=0.4, local_blend=lb)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3GSq67guHQNZ"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A painting of a squirrel eating a lasagne\"]\n",
        "lb = LocalBlend(prompts, (\"burger\", \"lasagne\"))\n",
        "controller = AttentionReplace(prompts, NUM_DIFFUSION_STEPS,\n",
        "                              cross_replace_steps={\"default_\": 1., \"lasagne\": .2},\n",
        "                              self_replace_steps=0.4,\n",
        "                              local_blend=lb)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "tags": [],
        "id": "wCNfmdxyHQNa"
      },
      "source": [
        "## Refinement edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "X6Uz3drEHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"A painting of a squirrel eating a burger\",\n",
        "           \"A neoclassical painting of a squirrel eating a burger\"]\n",
        "\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS,\n",
        "                             cross_replace_steps=.5,\n",
        "                             self_replace_steps=.2)\n",
        "_ = run_and_display(prompts, controller, latent=x_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "p4lO0B9YHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"a photo of a house on a mountain\",\n",
        "           \"a photo of a house on a mountain at fall\"]\n",
        "\n",
        "\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                             self_replace_steps=.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sSbJ1GJ4HQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"a photo of a house on a mountain\",\n",
        "           \"a photo of a house on a mountain at winter\"]\n",
        "\n",
        "\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                             self_replace_steps=.4)\n",
        "_ = run_and_display(prompts, controller, latent=x_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN97Onq-HQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"soup\",\n",
        "           \"pea soup\"]\n",
        "\n",
        "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
        "\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                             self_replace_steps=.4,\n",
        "                             local_blend=lb)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb80EtFUHQNa"
      },
      "source": [
        "## Attention Re-Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erILCJLoHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"a smiling bunny doll\"] * 2\n",
        "\n",
        "### pay 3 times more attention to the word \"smiling\"\n",
        "equalizer = get_equalizer(prompts[1], (\"smiling\",), (5,))\n",
        "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                               self_replace_steps=.4,\n",
        "                               equalizer=equalizer)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J49ZwSgnHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"pink bear riding a bicycle\"] * 2\n",
        "\n",
        "### we don't wont pink bikes, only pink bear.\n",
        "### we reduce the amount of pink but apply it locally on the bikes (attention re-weight + local mask )\n",
        "\n",
        "### pay less attention to the word \"pink\"\n",
        "equalizer = get_equalizer(prompts[1], (\"pink\",), (-1,))\n",
        "\n",
        "### apply the edit on the bikes\n",
        "lb = LocalBlend(prompts, (\"bicycle\", \"bicycle\"))\n",
        "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                               self_replace_steps=.4,\n",
        "                               equalizer=equalizer,\n",
        "                               local_blend=lb)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TGWSM0JHQNa"
      },
      "source": [
        "### Where are my croutons?\n",
        "It might be useful to use Attention Re-Weighting with a previous edit method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsNa0wTBHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"soup\",\n",
        "           \"pea soup with croutons\"]\n",
        "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                             self_replace_steps=.4, local_blend=lb)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Cd3_iS4HQNa"
      },
      "source": [
        "Now, with more attetnion to `\"croutons\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xELJrDdkHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"soup\",\n",
        "           \"pea soup with croutons\"]\n",
        "\n",
        "\n",
        "lb = LocalBlend(prompts, (\"soup\", \"soup\"))\n",
        "controller_a = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                               self_replace_steps=.4, local_blend=lb)\n",
        "\n",
        "### pay 3 times more attention to the word \"croutons\"\n",
        "equalizer = get_equalizer(prompts[1], (\"croutons\",), (3,))\n",
        "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
        "                               controller=controller_a)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nXDYf63HQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"potatos\",\n",
        "           \"fried potatos\"]\n",
        "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                             self_replace_steps=.4, local_blend=lb)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utS7Zz9pHQNa"
      },
      "outputs": [],
      "source": [
        "prompts = [\"potatos\",\n",
        "           \"fried potatos\"]\n",
        "lb = LocalBlend(prompts, (\"potatos\", \"potatos\"))\n",
        "controller = AttentionRefine(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                             self_replace_steps=.4, local_blend=lb)\n",
        "\n",
        "### pay 10 times more attention to the word \"fried\"\n",
        "equalizer = get_equalizer(prompts[1], (\"fried\",), (10,))\n",
        "controller = AttentionReweight(prompts, NUM_DIFFUSION_STEPS, cross_replace_steps=.8,\n",
        "                               self_replace_steps=.4, equalizer=equalizer, local_blend=lb,\n",
        "                               controller=controller_a)\n",
        "_ = run_and_display(prompts, controller, latent=x_t, run_baseline=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6FEaU16HQNa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "pytorch-gpu.1-11.m94",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}